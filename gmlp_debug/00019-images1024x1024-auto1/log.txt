Loading training set...

Num images:  70000
Image shape: [3, 1024, 1024]
Label shape: [0]

Constructing networks...
Setting up PyTorch plugin "bias_act_plugin"... Done.
before input torch.Size([4, 512, 4, 4])
after embed torch.Size([4, 16, 512])
inside block torch.Size([4, 16, 512])
after gmlp torch.Size([4, 16, 512])
after to img torch.Size([4, 512, 4, 4])
before input torch.Size([4, 512, 4, 4])
after embed torch.Size([4, 16, 512])
inside block torch.Size([4, 16, 512])
Traceback (most recent call last):
  File "train.py", line 538, in <module>
    main() # pylint: disable=no-value-for-parameter
  File "/home/ubuntu/anaconda3/envs/pytorch_180/lib/python3.6/site-packages/click/core.py", line 722, in __call__
    return self.main(*args, **kwargs)
  File "/home/ubuntu/anaconda3/envs/pytorch_180/lib/python3.6/site-packages/click/core.py", line 697, in main
    rv = self.invoke(ctx)
  File "/home/ubuntu/anaconda3/envs/pytorch_180/lib/python3.6/site-packages/click/core.py", line 895, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/ubuntu/anaconda3/envs/pytorch_180/lib/python3.6/site-packages/click/core.py", line 535, in invoke
    return callback(*args, **kwargs)
  File "/home/ubuntu/anaconda3/envs/pytorch_180/lib/python3.6/site-packages/click/decorators.py", line 17, in new_func
    return f(get_current_context(), *args, **kwargs)
  File "train.py", line 531, in main
    subprocess_fn(rank=0, args=args, temp_dir=temp_dir)
  File "train.py", line 383, in subprocess_fn
    training_loop.training_loop(rank=rank, **args)
  File "/home/ubuntu/stylegan2-ada-pytorch/training/training_loop.py", line 166, in training_loop
    img = misc.print_module_summary(G, [z, c])
  File "/home/ubuntu/stylegan2-ada-pytorch/torch_utils/misc.py", line 212, in print_module_summary
    outputs = module(*inputs)
  File "/home/ubuntu/anaconda3/envs/pytorch_180/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/ubuntu/stylegan2-ada-pytorch/training/networks.py", line 534, in forward
    img = self.synthesis(ws, **synthesis_kwargs)
  File "/home/ubuntu/anaconda3/envs/pytorch_180/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/ubuntu/stylegan2-ada-pytorch/training/networks.py", line 506, in forward
    x, img = block(x, img, cur_ws, **block_kwargs)
  File "/home/ubuntu/anaconda3/envs/pytorch_180/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/ubuntu/stylegan2-ada-pytorch/training/networks.py", line 440, in forward
    x = self.conv0(x, next(w_iter), fused_modconv=fused_modconv, **layer_kwargs)
  File "/home/ubuntu/anaconda3/envs/pytorch_180/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/ubuntu/stylegan2-ada-pytorch/training/networks.py", line 327, in forward
    x = self.gmlpblock(x)
  File "/home/ubuntu/anaconda3/envs/pytorch_180/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/ubuntu/stylegan2-ada-pytorch/training/g_mlp_pytorch.py", line 45, in forward
    return self.fn(x) + x
  File "/home/ubuntu/anaconda3/envs/pytorch_180/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/ubuntu/stylegan2-ada-pytorch/training/g_mlp_pytorch.py", line 74, in forward
    return self.fn(x, **kwargs)
  File "/home/ubuntu/anaconda3/envs/pytorch_180/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/ubuntu/stylegan2-ada-pytorch/training/g_mlp_pytorch.py", line 201, in forward
    x = self.sgu(x, gate_res = gate_res)
  File "/home/ubuntu/anaconda3/envs/pytorch_180/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/ubuntu/stylegan2-ada-pytorch/training/g_mlp_pytorch.py", line 163, in forward
    gate = einsum('b h n d, h m n -> b h m d', gate, weight)
  File "/home/ubuntu/anaconda3/envs/pytorch_180/lib/python3.6/site-packages/torch/functional.py", line 344, in einsum
    return _VF.einsum(equation, operands)  # type: ignore
RuntimeError: size of dimension does not match previous size, operand 1, dim 2
