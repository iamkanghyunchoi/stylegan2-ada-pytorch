Loading training set...

Num images:  70000
Image shape: [3, 1024, 1024]
Label shape: [0]

Constructing networks...
16
64
64
256
256
1024
1024
4096
4096
16384
16384
Setting up PyTorch plugin "bias_act_plugin"... Done.
before input torch.Size([4, 512, 4, 4])
after embed torch.Size([4, 16, 512])
gate, weight torch.Size([4, 1, 16, 1024]) torch.Size([1, 16, 16])
after gmlp torch.Size([4, 16, 512])
after to img torch.Size([4, 512, 4, 4])
Setting up PyTorch plugin "upfirdn2d_plugin"... Done.
before input torch.Size([4, 512, 8, 8])
after embed torch.Size([4, 64, 512])
gate, weight torch.Size([4, 1, 64, 1024]) torch.Size([1, 64, 64])
after gmlp torch.Size([4, 64, 512])
after to img torch.Size([4, 512, 8, 8])
before input torch.Size([4, 512, 8, 8])
after embed torch.Size([4, 64, 512])
gate, weight torch.Size([4, 1, 64, 1024]) torch.Size([1, 64, 64])
after gmlp torch.Size([4, 64, 512])
after to img torch.Size([4, 512, 8, 8])
before input torch.Size([4, 512, 16, 16])
after embed torch.Size([4, 256, 512])
gate, weight torch.Size([4, 1, 256, 1024]) torch.Size([1, 256, 256])
after gmlp torch.Size([4, 256, 512])
after to img torch.Size([4, 512, 16, 16])
before input torch.Size([4, 512, 16, 16])
after embed torch.Size([4, 256, 512])
gate, weight torch.Size([4, 1, 256, 1024]) torch.Size([1, 256, 256])
after gmlp torch.Size([4, 256, 512])
after to img torch.Size([4, 512, 16, 16])
before input torch.Size([4, 512, 32, 32])
after embed torch.Size([4, 1024, 512])
gate, weight torch.Size([4, 1, 1024, 1024]) torch.Size([1, 1024, 1024])
after gmlp torch.Size([4, 1024, 512])
after to img torch.Size([4, 512, 32, 32])
before input torch.Size([4, 512, 32, 32])
after embed torch.Size([4, 1024, 512])
gate, weight torch.Size([4, 1, 1024, 1024]) torch.Size([1, 1024, 1024])
after gmlp torch.Size([4, 1024, 512])
after to img torch.Size([4, 512, 32, 32])
before input torch.Size([4, 512, 64, 64])
after embed torch.Size([4, 4096, 512])
gate, weight torch.Size([4, 1, 4096, 1024]) torch.Size([1, 4096, 4096])
after gmlp torch.Size([4, 4096, 512])
after to img torch.Size([4, 512, 64, 64])
before input torch.Size([4, 512, 64, 64])
after embed torch.Size([4, 4096, 512])
gate, weight torch.Size([4, 1, 4096, 1024]) torch.Size([1, 4096, 4096])
after gmlp torch.Size([4, 4096, 512])
after to img torch.Size([4, 512, 64, 64])
before input torch.Size([4, 256, 128, 128])
Traceback (most recent call last):
  File "train.py", line 538, in <module>
    main() # pylint: disable=no-value-for-parameter
  File "/home/ubuntu/anaconda3/envs/pytorch_180/lib/python3.6/site-packages/click/core.py", line 722, in __call__
    return self.main(*args, **kwargs)
  File "/home/ubuntu/anaconda3/envs/pytorch_180/lib/python3.6/site-packages/click/core.py", line 697, in main
    rv = self.invoke(ctx)
  File "/home/ubuntu/anaconda3/envs/pytorch_180/lib/python3.6/site-packages/click/core.py", line 895, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/ubuntu/anaconda3/envs/pytorch_180/lib/python3.6/site-packages/click/core.py", line 535, in invoke
    return callback(*args, **kwargs)
  File "/home/ubuntu/anaconda3/envs/pytorch_180/lib/python3.6/site-packages/click/decorators.py", line 17, in new_func
    return f(get_current_context(), *args, **kwargs)
  File "train.py", line 531, in main
    subprocess_fn(rank=0, args=args, temp_dir=temp_dir)
  File "train.py", line 383, in subprocess_fn
    training_loop.training_loop(rank=rank, **args)
  File "/home/ubuntu/stylegan2-ada-pytorch/training/training_loop.py", line 166, in training_loop
    img = misc.print_module_summary(G, [z, c])
  File "/home/ubuntu/stylegan2-ada-pytorch/torch_utils/misc.py", line 212, in print_module_summary
    outputs = module(*inputs)
  File "/home/ubuntu/anaconda3/envs/pytorch_180/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/ubuntu/stylegan2-ada-pytorch/training/networks.py", line 536, in forward
    img = self.synthesis(ws, **synthesis_kwargs)
  File "/home/ubuntu/anaconda3/envs/pytorch_180/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/ubuntu/stylegan2-ada-pytorch/training/networks.py", line 508, in forward
    x, img = block(x, img, cur_ws, **block_kwargs)
  File "/home/ubuntu/anaconda3/envs/pytorch_180/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/ubuntu/stylegan2-ada-pytorch/training/networks.py", line 442, in forward
    x = self.conv0(x, next(w_iter), fused_modconv=fused_modconv, **layer_kwargs)
  File "/home/ubuntu/anaconda3/envs/pytorch_180/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/ubuntu/stylegan2-ada-pytorch/training/networks.py", line 335, in forward
    x = self.to_embed(x)
  File "/home/ubuntu/anaconda3/envs/pytorch_180/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/ubuntu/anaconda3/envs/pytorch_180/lib/python3.6/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/ubuntu/anaconda3/envs/pytorch_180/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/ubuntu/anaconda3/envs/pytorch_180/lib/python3.6/site-packages/torch/nn/modules/linear.py", line 93, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/ubuntu/anaconda3/envs/pytorch_180/lib/python3.6/site-packages/torch/nn/functional.py", line 1692, in linear
    output = input.matmul(weight.t())
RuntimeError: mat1 dim 1 must match mat2 dim 0
